{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vodafone Challenge\n",
    "## Scrap notebook\n",
    "- **Separate each test** you make with a markdown cell like this one (press M when the cursor is on a cell but it is not editing). \n",
    "- Put a **small description** on what you are doing and why you do so (like if you manipulate data in a specific way, or apply a particular definition of distance, write the intuition behind. Both for you to remmember later and for team members)\n",
    "- Make sure you are working with the **proper data** i.e. the data (and their transformation) that you with to use are defined before you do the analysis. Bugs could appear if you do not define something and Python retrieves older values for the variables you are using.\n",
    "- **Do not modify df_backup**, always work with a copy [like df = df_backup.copy()]\n",
    "- Add short line of description in the Summary section\n",
    "- For each test, write briefly which value of the parameter tried (like learning rate constant, tried eta0 large (10^-2) not well, smaller (10^-7) seem to work best. Then changed with learning rate adaptivive [which?] and tried ... large (10^-2) worked best).\n",
    "\n",
    "**For the best test, build pipeline: bulleted version of all things done on the dataset until the result. It could be a useful thing to do for each test actually**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. **Title of the test**: description of the test (briefly, just to quickly know what tryed already)\n",
    "2. **Sample 2**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_backup = pd.read_csv('dataset_challenge_v5.TRAINING.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baldassi's Cleaning\n",
    "**DeviceOperatingSystem**: I preferred not to create a specific category for 'windows' because too few observations, however, if the 'other' category reveals to explain well, we can unpack it (in a new dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iOS': 568, nan: 548, 'Android': 858, 'Windows Mobile': 6, 'Proprietary': 5, 'Windows Phone': 7, 'BlackBerry OS': 1, 'Firefox': 1, 'Symbian^3': 1, 'BREW': 1, 'Series 40': 2, 'BB10': 1, 'VRTXmc': 1}\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_backup.copy()\n",
    "\n",
    "del df_clean['Unnamed: 0']\n",
    "\n",
    "c = list(df_clean.columns)\n",
    "c[0] = 'ID'\n",
    "df_clean.columns = c\n",
    "\n",
    "df_clean['ZipCode'] = df_clean['ZipCode'].map(lambda x: '%05i' % x, na_action='ignore')\n",
    "\n",
    "traffic_columns = ['File-Transfer', 'Games',\n",
    "       'Instant-Messaging-Applications', 'Mail', 'Music-Streaming',\n",
    "       'Network-Operation', 'P2P-Applications', 'Security',\n",
    "       'Streaming-Applications', 'Terminals', 'Unclassified', 'VoIP',\n",
    "       'Web-Applications']\n",
    "df_clean[traffic_columns]\n",
    "\n",
    "cats = df_clean['CustomerAge'].astype('category').cat.categories\n",
    "d = {cat:(15+10*i)/100 for i,cat in enumerate(cats)}\n",
    "df_clean['NumericAge'] = df_clean['CustomerAge'].map(lambda x: d[x], na_action='ignore')\n",
    "\n",
    "d = {}\n",
    "for elem in df_clean['DeviceOperatingSystem']:\n",
    "    d[elem] = d.get(elem, 0) + 1\n",
    "print(d) #some categories have very few values, group them\n",
    "OS_other = []\n",
    "for key in d:\n",
    "    if d[key] < 10:\n",
    "        OS_other.append(key)\n",
    "        d[key] = 'other'\n",
    "    else:\n",
    "        d[key] = key\n",
    "df_clean['OS_clean'] = df_clean['DeviceOperatingSystem'].map(lambda x: d[x], na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class definition and useful dependencies\n",
    "Space that collects classes or function definition that come in handy throughtout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class buildTrain():\n",
    "    def __init__(self, X, y, perc=0.8, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        n_data, n_features = X.shape\n",
    "        assert n_data == len(y)\n",
    "        \n",
    "        perm = np.random.permutation(n_data)\n",
    "        n_train = int(n_data * perc)\n",
    "        train_mask = perm[:n_train]\n",
    "        valid_mask = perm[n_train:]\n",
    "        \n",
    "        assert (len(train_mask)+len(valid_mask)) == n_data\n",
    "        \n",
    "        train_data = X[train_mask]\n",
    "        train_target = y[train_mask]\n",
    "        valid_data = X[valid_mask]\n",
    "        valid_target = y[valid_mask]\n",
    "        assert (len(train_data)+len(valid_data)) == n_data\n",
    "        \n",
    "        self.Xt = train_data\n",
    "        self.yt = train_target\n",
    "        self.Xv = valid_data\n",
    "        self.yv = valid_target\n",
    "        \n",
    "    def get_train(self):\n",
    "        return self.Xt, self.yt\n",
    "    \n",
    "    def get_valid(self):\n",
    "        return self.Xv, self.yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_clusters(y, clust_labels, img_threshold=15):\n",
    "    #checks input\n",
    "    if y.ndim != 1: \n",
    "        raise Exception(2)\n",
    "    if len(y) != len(clust_labels):\n",
    "        raise Exception(4)\n",
    "        \n",
    "    #build histogram of categories (how many point for each cat)\n",
    "    cats = {}\n",
    "    for i in y:\n",
    "        cats[i] = cats.get(i, 0) + 1\n",
    "    n_cats = len(cats)\n",
    "    print(cats)\n",
    "    \n",
    "    #build histogram of clusters (how many point in each cluster)\n",
    "    clusters = {}\n",
    "    for i in clust_labels:\n",
    "        clusters[i] = clusters.get(i, 0) + 1\n",
    "    n_clusters = len(clusters)\n",
    "    print(clusters)\n",
    "        \n",
    "    #create mapping from categories to index (to easily store data)\n",
    "    #done because we assume y's values can be different from range(n_categories)\n",
    "    #cat_list useful to quickly go back (header of result matrix)\n",
    "    cat_map = {}\n",
    "    cat_list = []\n",
    "    for i, cat in enumerate(cats):\n",
    "        cat_map[cat] = i\n",
    "        cat_list.append(cat)\n",
    "    \n",
    "    #for each cluster, computes proportion of point belonging to each category\n",
    "    result = np.zeros((n_clusters, n_cats))\n",
    "    tot_per_clust = np.zeros((n_clusters,1), dtype=int)\n",
    "    for i, clust in enumerate(clusters):\n",
    "        labels = y[clust_labels == clust]\n",
    "        tot_per_clust[i] = clusters[clust]\n",
    "        for cat in labels:\n",
    "            result[i,cat_map[cat]] += 1\n",
    "    \n",
    "    #express each value as a proportion (normalization)\n",
    "    print(result)\n",
    "    print(tot_per_clust)\n",
    "    result = result / tot_per_clust * 100\n",
    "    print(np.sum(result, axis=1), n_clusters)\n",
    "    \n",
    "    #show graphical representation if matrix not too big\n",
    "    if n_cats < img_threshold and n_clusters < img_threshold:\n",
    "        plt.imshow(result)\n",
    "        \n",
    "    #for each cluster show the category that fits it best\n",
    "    for i,value in enumerate(np.argmax(result, axis=1)):\n",
    "        print('cluster: %s --> top category: %s, frequency of category: %.2f%%' % (i, cat_list[value], result[i, value]))\n",
    "    print('Overall score: %.2f%%'%(np.sum(np.max(result, axis=1))/n_clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
