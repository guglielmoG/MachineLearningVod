{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vodafone Challenge\n",
    "## Scrap notebook\n",
    "- **Separate each test** you make with a markdown cell like this one (press M when the cursor is on a cell but it is not editing). \n",
    "- Put a **small description** on what you are doing and why you do so (like if you manipulate data in a specific way, or apply a particular definition of distance, write the intuition behind. Both for you to remmember later and for team members)\n",
    "- Make sure you are working with the **proper data** i.e. the data (and their transformation) that you with to use are defined before you do the analysis. Bugs could appear if you do not define something and Python retrieves older values for the variables you are using.\n",
    "- **Do not modify df_backup**, always work with a copy [like df = df_backup.copy()]\n",
    "- Add short line of description in the Summary section\n",
    "- For each test, write briefly which value of the parameter tried (like learning rate constant, tried eta0 large (10^-2) not well, smaller (10^-7) seem to work best. Then changed with learning rate adaptivive [which?] and tried ... large (10^-2) worked best).\n",
    "\n",
    "**For the best test, build pipeline: bulleted version of all things done on the dataset until the result. It could be a useful thing to do for each test actually**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. **K-means on traffic data**: tested for different k's, both standardized and not. Performs poorly.\n",
    "2. **preprocessing zip-urb** (Abetone, Montoro, Ginosa, Capannori, Vigo di Fassa, Scarpiera e San Piero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "### *setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(threshold=np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = pd.read_csv('dataset_challenge_v5.TRAINING.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "**DeviceOperatingSystem**: I preferred not to create a specific category for 'windows' because too few observations, however, if the 'other' category reveals to explain well, we can unpack it (in a new dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iOS': 568, nan: 548, 'Android': 858, 'Windows Mobile': 6, 'Proprietary': 5, 'Windows Phone': 7, 'BlackBerry OS': 1, 'Firefox': 1, 'Symbian^3': 1, 'BREW': 1, 'Series 40': 2, 'BB10': 1, 'VRTXmc': 1}\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_backup.copy()\n",
    "\n",
    "del df_clean['Unnamed: 0']\n",
    "\n",
    "c = list(df_clean.columns)\n",
    "c[0] = 'ID'\n",
    "df_clean.columns = c\n",
    "\n",
    "df_clean['ZipCode'] = df_clean['ZipCode'].map(lambda x: '%05i' % x, na_action='ignore')\n",
    "\n",
    "traffic_columns = ['File-Transfer', 'Games',\n",
    "       'Instant-Messaging-Applications', 'Mail', 'Music-Streaming',\n",
    "       'Network-Operation', 'P2P-Applications', 'Security',\n",
    "       'Streaming-Applications', 'Terminals', 'Unclassified', 'VoIP',\n",
    "       'Web-Applications']\n",
    "df_clean[traffic_columns]\n",
    "\n",
    "cats = df_clean['CustomerAge'].astype('category').cat.categories\n",
    "d = {cat:(15+10*i)/100 for i,cat in enumerate(cats)}\n",
    "df_clean['NumericAge'] = df_clean['CustomerAge'].map(lambda x: d[x], na_action='ignore')\n",
    "\n",
    "d = {}\n",
    "for elem in df_clean['DeviceOperatingSystem']:\n",
    "    d[elem] = d.get(elem, 0) + 1\n",
    "print(d) #some categories have very few values, group them\n",
    "OS_other = []\n",
    "for key in d:\n",
    "    if d[key] < 10:\n",
    "        OS_other.append(key)\n",
    "        d[key] = 'other'\n",
    "    else:\n",
    "        d[key] = key\n",
    "df_clean['OS_clean'] = df_clean['DeviceOperatingSystem'].map(lambda x: d[x], na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Renato/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Adding rural/urban information\n",
    "df_zip_istat = pd.read_csv('databases/database.csv')\n",
    "df_istat_urb = pd.read_csv('databases/it_postal_codes.csv/Foglio 2-Tabella 1.csv', error_bad_lines=False, sep = ';')\n",
    "my_urb_dict = {'Basso' : 0, 'Medio' : 1, 'Elevato' : 2}\n",
    "df_istat_urb['GradoUrbaniz'] = df_istat_urb['GradoUrbaniz'].map(lambda x: my_urb_dict[x], na_action = 'ignore')\n",
    "\n",
    "#check there are no datapoint for which we don't have zip but we've region\n",
    "df_clean['ZipCode'].isnull()\n",
    "df_clean['Region'][df_clean['ZipCode'].isnull()]\n",
    "len(df_clean['Region'][df_clean['ZipCode'].isnull()]) == np.sum(df_clean['Region'][df_clean['ZipCode'].isnull()].isnull())\n",
    "\n",
    "#we need to insert x for multiple cap cities\n",
    "isnan = lambda x: x != x\n",
    "#nan is unique type not equal to itself, so with this lambda function we get True only when the type is NaN\n",
    "\n",
    "for i in range(df_zip_istat.shape[0]):\n",
    "    cap = df_zip_istat.loc[i, 'cap/0']\n",
    "    cap  = '%05d' % cap\n",
    "    if not isnan(df_zip_istat.loc[i,'cap/1']):\n",
    "        if not isnan(df_zip_istat.loc[i,'cap/10']):   \n",
    "            cap = cap[:-2]+'xx'\n",
    "        else:\n",
    "            cap = cap[:-1]+'x'\n",
    "    df_zip_istat.loc[i, 'cap/0'] = cap\n",
    "\n",
    "d_zip_istat = df_zip_istat.set_index('cap/0').to_dict()['codice']\n",
    "d_istat_urb = df_istat_urb.set_index('ISTAT').to_dict()['GradoUrbaniz']\n",
    "\n",
    "mask = df_clean['ZipCode'].isnull()\n",
    "urban_col = np.zeros(df_clean.shape[0])\n",
    "urban_col_masked = urban_col[~ mask]\n",
    "d_zip_istat.update([('51021', 47023),( '83026', 64121),( '74025', 73007),( '55062', 46007),( '38039', 22217),('50037', 48053)])\n",
    "d_istat_urb.update([(22250, 0),( 78157, 1)])\n",
    "\n",
    "c = 0\n",
    "for i in df_clean['ZipCode'][~ mask]:\n",
    "    try:\n",
    "        temp = d_zip_istat[i]\n",
    "        urban_col_masked[c] = d_istat_urb[int(temp)]\n",
    "    except KeyError:\n",
    "        i = '%05d' % int(i)\n",
    "        if i[:-1]+'x' in d_zip_istat:\n",
    "            temp = d_zip_istat[i[:-1]+'x']\n",
    "        elif i[:-2]+'xx' in d_zip_istat:\n",
    "            temp = d_zip_istat[i[:-2]+'xx']\n",
    "        else:\n",
    "            raise()\n",
    "    c += 1\n",
    "    \n",
    "df_clean['Urban'] = df_clean['ZipCode'].copy()\n",
    "df_clean['Urban'][~ mask] = urban_col_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class definition and useful dependencies\n",
    "Space that collects classes or function definition that come in handy throughtout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class buildTrain():\n",
    "    def __init__(self, X, y, perc=0.8, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        n_data, n_features = X.shape\n",
    "        assert n_data == len(y)\n",
    "        \n",
    "        perm = np.random.random(n_data)\n",
    "        train_mask = perm < perc\n",
    "        valid_mask = ~ train_mask\n",
    "        \n",
    "        train_data = X[train_mask]\n",
    "        train_target = y[train_mask]\n",
    "        valid_data = X[valid_mask]\n",
    "        valid_target = y[valid_mask]\n",
    "        assert (len(train_data)+len(valid_data)) == n_data\n",
    "        \n",
    "        self.Xt = train_data\n",
    "        self.yt = train_target\n",
    "        self.Xv = valid_data\n",
    "        self.yv = valid_target\n",
    "        \n",
    "    def get_train(self):\n",
    "        return self.Xt, self.yt\n",
    "    \n",
    "    def get_valid(self):\n",
    "        return self.Xv, self.yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logger():\n",
    "    def __init__(self, verbose = True):\n",
    "        self.v = verbose\n",
    "        self.log_ = []\n",
    "        \n",
    "    def log_it(self, text):\n",
    "        #adds to log record\n",
    "        if not isinstance(text, str):\n",
    "            raise Exception('must pass text to logger')\n",
    "        if self.v:\n",
    "            print(text)\n",
    "        self.log_.append(text)\n",
    "        \n",
    "    def print_out(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            raise Exception('must pass text to logger')\n",
    "        #doesn't add to log record\n",
    "        if self.v:\n",
    "            print(text)\n",
    "        \n",
    "    def show_img(self, array):\n",
    "        if not isinstance(array, np.ndarray):\n",
    "            raise Exception(1)\n",
    "        if self.v:\n",
    "            plt.imshow(array)\n",
    "        \n",
    "    def get_log(self):\n",
    "        return \"\\n\".join(self.log_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clusters(y, clust_labels, img_threshold=15, v=True):\n",
    "    #checks input\n",
    "    if y.ndim != 1: \n",
    "        raise Exception(2)\n",
    "    if len(y) != len(clust_labels):\n",
    "        raise Exception(4)\n",
    "    \n",
    "    #logger setup\n",
    "    my_log = logger(verbose=v)\n",
    "        \n",
    "    #build histogram of categories (how many point for each cat)\n",
    "    cats = {}\n",
    "    for i in y:\n",
    "        cats[i] = cats.get(i, 0) + 1\n",
    "    n_cats = len(cats)\n",
    "    \n",
    "    #build histogram of clusters (how many point in each cluster)\n",
    "    clusters = {}\n",
    "    for i in clust_labels:\n",
    "        clusters[i] = clusters.get(i, 0) + 1\n",
    "    n_clusters = len(clusters)\n",
    "        \n",
    "    #create mapping from categories to index (to easily store data)\n",
    "    #done because we assume y's values can be different from range(n_categories)\n",
    "    #cat_list useful to quickly go back (header of result matrix)\n",
    "    cat_map = {}\n",
    "    cat_list = []\n",
    "    for i, cat in enumerate(cats):\n",
    "        cat_map[cat] = i\n",
    "        cat_list.append(cat)\n",
    "    \n",
    "    #for each cluster, computes proportion of point belonging to each category\n",
    "    result = np.zeros((n_clusters, n_cats))\n",
    "    tot_per_clust = np.zeros((n_clusters,1), dtype=int)\n",
    "    for i, clust in enumerate(clusters):\n",
    "        labels = y[clust_labels == clust]\n",
    "        tot_per_clust[i] = clusters[clust]\n",
    "        for cat in labels:\n",
    "            result[i,cat_map[cat]] += 1\n",
    "            \n",
    "    #to compute percentage of category points\n",
    "    perc_cat = []\n",
    "    for clust in range(len(result)):\n",
    "        i_max = np.argmax(result[clust,:])\n",
    "        tot = cats[cat_list[i_max]]\n",
    "        perc_cat.append(result[clust, i_max] / tot * 100)\n",
    "        \n",
    "    #express each value as a proportion (normalization)\n",
    "    result = result / tot_per_clust * 100\n",
    "    \n",
    "    #show graphical representation if matrix not too big\n",
    "    if n_cats < img_threshold and n_clusters < img_threshold:\n",
    "        my_log.show_img(result)\n",
    "        \n",
    "    #for each cluster show the category that fits it best\n",
    "    for i,value in enumerate(np.argmax(result, axis=1)):\n",
    "        #frequency of category: number of datapoint of a specific category belonging to that cluster\n",
    "        #over the number of points in the cluster (variety within cluster)\n",
    "        #category clustering: number of datapoint of a specific category belonging to that cluster,\n",
    "        #over the total number of points of that category\n",
    "        my_log.log_it('cluster: %s --> top category: %s, frequency of category (variety within cluster): %.2f%%, category clustering: %.2f%%'\\\n",
    "              % (i, cat_list[value], result[i, value], perc_cat[i]))\n",
    "    score = np.sum(np.max(result, axis=1))/n_clusters\n",
    "    weighted = np.dot(np.max(result, axis=1), np.array(perc_cat))/100\n",
    "    #maybe it's best to weight the score by the category clustering index (see k-means example below)\n",
    "    my_log.log_it(\"Overall score (doesn't consider category clustering): %.2f%%, weighted: %.2f%%\"%(score, weighted))\n",
    "    return weighted, my_log.get_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, column):\n",
    "    if not isinstance(column, (str, int)):\n",
    "        raise Exception(1)\n",
    "    #returns a copy of the standardized column\n",
    "    c = df[column].copy()\n",
    "    mean = c.mean()\n",
    "    sd = c.std()\n",
    "    return (c - mean) / sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_std(df, columns):\n",
    "    if not isinstance(columns, str):\n",
    "        if len(columns) == 0:\n",
    "            raise Exception('nto enough columns')\n",
    "    else:\n",
    "        raise Exception('must be an array or list')\n",
    "    #returns a new dataframe with standardized columns\n",
    "    new_df = pd.DataFrame()\n",
    "    for column in columns:\n",
    "        temp = standardize(df, column)\n",
    "        new_df[column] = temp\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test():\n",
    "    def __init__(self):\n",
    "        self.best = [0,0,0]\n",
    "        self.scores = []\n",
    "        \n",
    "    def update(self, score, k):\n",
    "        score, log = score\n",
    "        if score > self.best[0]:\n",
    "            self.best = score, k, log\n",
    "        self.scores.append((k, score))\n",
    "        \n",
    "    def get_result(self):\n",
    "        best = self.best\n",
    "        scores = self.scores\n",
    "        print('best weighted score: %.2f%%, number of clusters: %i' % (best[0], best[1]))\n",
    "        print('log of best: \\n%s' % best[2])\n",
    "        plt.figure()\n",
    "        plt.plot(*zip(*scores),'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means on traffic\n",
    "Just an exploratory study, let's see what we get..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = df_clean[traffic_columns]\n",
    "y = df_clean['Product']\n",
    "\n",
    "km = KMeans(n_clusters=6, init='k-means++', n_init=10, n_jobs=4)\n",
    "km.fit(X)\n",
    "score = check_clusters(y=y, clust_labels=km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we try the same but with standardized columns, see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = batch_std(df_clean, traffic_columns)\n",
    "y = df_clean['Product']\n",
    "\n",
    "km = KMeans(n_clusters=6, init='k-means++', n_init=10, n_jobs=4)\n",
    "km.fit(X_std)\n",
    "score = check_clusters(y=y, clust_labels=km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to have improved. Still the clusters seem to separate pretty poorly. Let's see the optimal value of k based on our previous score (using standardized data which seem to make more sense). We set the same seed each time so that the results are comparable and not influenced by different initial centroid allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test = test()\n",
    "\n",
    "np.random.seed(23453)\n",
    "X_std = batch_std(df_clean, traffic_columns)\n",
    "y = df_clean['Product']\n",
    "\n",
    "for k in range(2, 10):\n",
    "    km = KMeans(n_clusters=k, init='k-means++', n_init=10, n_jobs=4)\n",
    "    km.fit(X_std)\n",
    "    \n",
    "    score= check_clusters(y=y, clust_labels=km.labels_, v=False)\n",
    "    my_test.update(score, k)\n",
    "        \n",
    "my_test.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering on traffic data\n",
    "Same approach as for k-means, test difference between standardized and not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "my_test = test()\n",
    "X_std = batch_std(df_clean, traffic_columns)\n",
    "y = df_clean['Product']\n",
    "\n",
    "for k in range(2, 8):\n",
    "    clust = AgglomerativeClustering(n_clusters=k, linkage='ward', affinity='euclidean')\n",
    "    clust.fit(X_std)\n",
    "    score = check_clusters(y=y, clust_labels=clust.labels_, v=False)\n",
    "    my_test.update(score, k)\n",
    "\n",
    "my_test.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try complete linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "my_test = test()\n",
    "X_std = batch_std(df_clean, traffic_columns)\n",
    "y = df_clean['Product']\n",
    "\n",
    "for k in range(2, 8):\n",
    "    clust = AgglomerativeClustering(n_clusters=k, linkage='complete', affinity='euclidean')\n",
    "    clust.fit(X_std)\n",
    "    score = check_clusters(y=y, clust_labels=clust.labels_, v=False)\n",
    "    my_test.update(score, k)\n",
    "\n",
    "my_test.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron (imputation of age data with traffic data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we try to build a Perceptron to complete the age column keeping all the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def train_perc(X, y, percentage = 0.8, seed = None, max_iter = 5000, it_interval = 100, **args):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    epochs = max_iter // it_interval\n",
    "    data = buildTrain(X, y, percentage)\n",
    "\n",
    "    \n",
    "    n_features = X.shape[1]\n",
    "    n_classes = len(y.unique())\n",
    "    \n",
    "    coef = np.random.randn(n_classes, n_features) * 1e-2\n",
    "    intercept = np.random.randn(n_classes) * 1e-2\n",
    "    eta = args[\"eta0\"]\n",
    "    \n",
    "    tscores = []\n",
    "    vscores = []\n",
    "    for epoch in range(epochs):\n",
    "        perc = SGDClassifier(**args)\n",
    "        perc.fit(*data.get_train(), coef_init = coef, intercept_init = intercept)\n",
    "        tscore = perc.score(*data.get_train())\n",
    "        vscore = perc.score(*data.get_valid())\n",
    "        print(\"epoch=%i tscore=%g vscore=%g\" % (epoch+1, tscore, vscore))\n",
    "        tscores.append(tscore)\n",
    "        vscores.append(vscore)\n",
    "        coef, intercept = perc.coef_, perc.intercept_\n",
    "        args[\"eta0\"] = eta / ((epoch + 1) * it_interval)**args['power_t']\n",
    "    \n",
    "    plt.plot(np.arange(epochs), tscores, np.arange(epochs), vscores)\n",
    "    \n",
    "    #return perc, data, tscores, vscores\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Renato/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 tscore=0.361059 vscore=0.399516\n",
      "epoch=2 tscore=0.361059 vscore=0.399516\n",
      "epoch=3 tscore=0.361059 vscore=0.399516\n",
      "epoch=4 tscore=0.361059 vscore=0.399516\n",
      "epoch=5 tscore=0.361059 vscore=0.399516\n",
      "epoch=6 tscore=0.361059 vscore=0.399516\n",
      "epoch=7 tscore=0.361059 vscore=0.399516\n",
      "epoch=8 tscore=0.361059 vscore=0.399516\n",
      "epoch=9 tscore=0.361059 vscore=0.399516\n",
      "epoch=10 tscore=0.361059 vscore=0.399516\n",
      "epoch=11 tscore=0.361059 vscore=0.399516\n",
      "epoch=12 tscore=0.361059 vscore=0.399516\n",
      "epoch=13 tscore=0.361059 vscore=0.399516\n",
      "epoch=14 tscore=0.361059 vscore=0.399516\n",
      "epoch=15 tscore=0.361059 vscore=0.399516\n",
      "epoch=16 tscore=0.361059 vscore=0.399516\n",
      "epoch=17 tscore=0.361059 vscore=0.399516\n",
      "epoch=18 tscore=0.361059 vscore=0.399516\n",
      "epoch=19 tscore=0.361059 vscore=0.399516\n",
      "epoch=20 tscore=0.361059 vscore=0.399516\n",
      "epoch=21 tscore=0.361059 vscore=0.399516\n",
      "epoch=22 tscore=0.361059 vscore=0.399516\n",
      "epoch=23 tscore=0.361059 vscore=0.399516\n",
      "epoch=24 tscore=0.361059 vscore=0.399516\n",
      "epoch=25 tscore=0.361059 vscore=0.399516\n",
      "epoch=26 tscore=0.361059 vscore=0.399516\n",
      "epoch=27 tscore=0.361059 vscore=0.399516\n",
      "epoch=28 tscore=0.361059 vscore=0.399516\n",
      "epoch=29 tscore=0.361059 vscore=0.399516\n",
      "epoch=30 tscore=0.361059 vscore=0.399516\n",
      "epoch=31 tscore=0.361059 vscore=0.399516\n",
      "epoch=32 tscore=0.361059 vscore=0.399516\n",
      "epoch=33 tscore=0.361059 vscore=0.399516\n",
      "epoch=34 tscore=0.361059 vscore=0.399516\n",
      "epoch=35 tscore=0.361059 vscore=0.399516\n",
      "epoch=36 tscore=0.361059 vscore=0.399516\n",
      "epoch=37 tscore=0.361059 vscore=0.399516\n",
      "epoch=38 tscore=0.361059 vscore=0.399516\n",
      "epoch=39 tscore=0.361059 vscore=0.399516\n",
      "epoch=40 tscore=0.361059 vscore=0.399516\n",
      "epoch=41 tscore=0.361059 vscore=0.399516\n",
      "epoch=42 tscore=0.361059 vscore=0.399516\n",
      "epoch=43 tscore=0.361059 vscore=0.399516\n",
      "epoch=44 tscore=0.361059 vscore=0.399516\n",
      "epoch=45 tscore=0.361059 vscore=0.399516\n",
      "epoch=46 tscore=0.361059 vscore=0.399516\n",
      "epoch=47 tscore=0.361059 vscore=0.399516\n",
      "epoch=48 tscore=0.361059 vscore=0.399516\n",
      "epoch=49 tscore=0.361059 vscore=0.399516\n",
      "epoch=50 tscore=0.361059 vscore=0.399516\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1fb5e438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_perc(df_clean[traffic_columns], df_clean['Product'], max_iter=50000, it_interval=1000, loss = 'log', penalty = \"none\", \n",
    "           alpha = 1e-2, learning_rate = 'invscaling', eta0 = 1e-6, warm_start = True, power_t = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
